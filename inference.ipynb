{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ae3e4d-25e0-4d75-8e4a-bc014e0c36c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 0: kill: (73666) - No such process\n"
     ]
    }
   ],
   "source": [
    "!kill -KILL 73666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d54a739-8e04-449c-bc10-9fe4010f1675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun  3 19:15:25 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.07             Driver Version: 535.161.07   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:21:00.0 Off |                  Off |\n",
      "| 30%   32C    P8              16W / 450W |      1MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3b3c43d-fad6-4b02-b0d7-85bdda42cb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 19:15:28,609\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.7.0 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from inference_chain import (\n",
    "    get_inference_arguments,\n",
    "    get_sampling_params,\n",
    "    get_lora_request,\n",
    "    format_data,\n",
    "    initialize_engine,\n",
    "    process_requests,\n",
    "    filter_out_results,\n",
    "    write_results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ce69e9d-a715-46f4-a35f-873a090ba5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from unsloth import FastLanguageModel\n",
    "#model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#    model_name = \"unsloth/gemma-2b-it\",\n",
    "#    max_seq_length = 1024,\n",
    "#    dtype = None,\n",
    "#    load_in_4bit = False\n",
    "#)\n",
    "#tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6917f9db-3666-4158-be54-df3c511e3607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from tqdm import tqdm\n",
    "from vllm import SamplingParams, LLMEngine, EngineArgs\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "from prompting import build_source_prompt, EOT_TOKEN, M1, MI, MA\n",
    "from arguments import get_inference_arguments\n",
    "from settings import OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "712ebd2a-0c94-4ce2-ab22-c704a1090498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chain_inference(m):\n",
    "\n",
    "    data_dir = \"/cluster/work/lawecon/Work/mbraasch/output/deepseek-7b-base-m1m2m3m4m58/data/test/normal/1.json\"\n",
    "    \n",
    "    print(f\"\\n*** Start inference for model {m} ***\")\n",
    "    args.run_name = os.path.join(run_name, f'm{m[0]}-{m[-1]}' if isinstance(m, list) else f\"m{m}\")\n",
    "    lora_request = get_lora_request(lora_path=args.run_name)\n",
    "    print(f\"Doing inference for model (at args.run_name) {args.run_name}.\")\n",
    "\n",
    "    instruction = M1 if m == 1 else (MA if isinstance(m, list) else MI)\n",
    "    print(f\"Using instruction: {instruction}\")\n",
    "\n",
    "    data = format_data(\n",
    "        data_dir=data_dir,\n",
    "        sampling_params=sampling_params,\n",
    "        instruction=instruction,\n",
    "        lora_request=lora_request,\n",
    "        target_path=target_path\n",
    "    )\n",
    "    engine = initialize_engine(args) \n",
    "    results = process_requests(engine, data)\n",
    "    results, finished = filter_out_results(results)\n",
    "    write_results(args, results, finished, run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccc6a7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_3_step_only_inference():\n",
    "    data_dir = \"/cluster/work/lawecon/Work/mbraasch/output/deepseek-7b-base-3-step-only/data/test_3_steps.json\"\n",
    "    print(f\"\\n*** Start inference for 3-step only model ***\")\n",
    "    args.run_name = os.path.join(OUTPUT_DIR, \"deepseek-7b-base-3-step-only\")\n",
    "    lora_request = get_lora_request(lora_path=args.run_name)\n",
    "    print(f\"Doing inference for model {args.run_name}.\")\n",
    "    instruction = MA\n",
    "    print(f\"Using instruction: {instruction}\")\n",
    "    data = format_data(\n",
    "        data_dir=data_dir,\n",
    "        sampling_params=sampling_params,\n",
    "        instruction=instruction,\n",
    "        lora_request=lora_request\n",
    "    )\n",
    "    engine = initialize_engine(args)\n",
    "    results = process_requests(engine, data)\n",
    "    results, finished = filter_out_results(results)\n",
    "    write_results(args, results, finished, run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10f213e0-74de-4972-892f-e90f4852ff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline_inference(run_name):\n",
    "    \n",
    "    # Params\n",
    "    delimiter = \"\\nFinal Answer: \"\n",
    "    sampling_params = get_sampling_params(eot_token=\"<eos>\")\n",
    "    data_dir = \"/cluster/work/lawecon/Work/mbraasch/data/formatted/test.json\"    \n",
    "    instruction = \"Solve the following math word problem step-by-step.\"#\"Solve the following Math Word Problem\"#MA\n",
    "    \n",
    "    # Run\n",
    "    args = get_inference_arguments()\n",
    "    args.run_name = os.path.join(OUTPUT_DIR, run_name)\n",
    "    lora_request = get_lora_request(lora_path=args.run_name)\n",
    "    data = format_data(\n",
    "        data_dir=data_dir,\n",
    "        sampling_params=sampling_params,\n",
    "        instruction=instruction,\n",
    "        lora_request=lora_request\n",
    "    )\n",
    "    engine = initialize_engine(args)\n",
    "    results = process_requests(engine, data)\n",
    "    results, finished = filter_out_results(results, delimiter=delimiter)\n",
    "    write_results(args, results, finished, args.run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67369191-441e-4ffe-abb8-0962cb6c3b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initalizating sampling params.\n",
      "Getting max adapter checkpoint from /cluster/work/lawecon/Work/mbraasch/output/dl_my_format.\n",
      "Loaded ckpt /cluster/work/lawecon/Work/mbraasch/output/dl_my_format/checkpoint-372/adapter_model.\n",
      "DP Load data from /cluster/work/lawecon/Work/mbraasch/data/formatted/test.json\n",
      "Format data.\n",
      "Initialize the LLMEngine.\n",
      "INFO 06-03 19:15:30 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='google/gemma-2b-it', speculative_config=None, tokenizer='google/gemma-2b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=google/gemma-2b-it)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/mbraasch/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 19:15:31 utils.py:660] Found nccl from library /cluster/home/mbraasch/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 06-03 19:15:31 selector.py:81] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 06-03 19:15:31 selector.py:32] Using XFormers backend.\n",
      "WARNING 06-03 19:15:32 gemma.py:54] Gemma's activation function was incorrectly set to exact GeLU in the config JSON file when it was initially released. Changing the activation function to approximate GeLU (`gelu_pytorch_tanh`). If you want to use the legacy `gelu`, edit the config JSON to set `hidden_activation=gelu` instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "INFO 06-03 19:15:32 weight_utils.py:199] Using model weights format ['*.safetensors']\n",
      "INFO 06-03 19:15:34 model_runner.py:175] Loading model weights took 4.6720 GB\n",
      "INFO 06-03 19:15:35 gpu_executor.py:114] # GPU blocks: 51817, # CPU blocks: 14563\n",
      "INFO 06-03 19:15:36 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-03 19:15:36 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-03 19:15:41 model_runner.py:1017] Graph capturing finished in 5 secs.\n",
      "Start processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1319 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 19:15:46 metrics.py:334] Avg prompt throughput: 5673.7 tokens/s, Avg generation throughput: 49.2 tokens/s, Running: 246 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 27/1319 [00:09<01:36, 13.42it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 19:15:51 metrics.py:334] Avg prompt throughput: 890.8 tokens/s, Avg generation throughput: 3936.3 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 66 reqs, GPU KV cache usage: 6.0%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 154/1319 [00:14<00:49, 23.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 19:15:56 metrics.py:334] Avg prompt throughput: 2822.6 tokens/s, Avg generation throughput: 3031.0 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 49 reqs, GPU KV cache usage: 6.3%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 255/1319 [00:19<00:50, 21.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 19:16:01 metrics.py:334] Avg prompt throughput: 2336.6 tokens/s, Avg generation throughput: 2986.5 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 55 reqs, GPU KV cache usage: 6.4%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 373/1319 [00:24<00:36, 25.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 19:16:06 metrics.py:334] Avg prompt throughput: 2672.4 tokens/s, Avg generation throughput: 2968.1 tokens/s, Running: 252 reqs, Swapped: 0 reqs, Pending: 45 reqs, GPU KV cache usage: 6.2%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 485/1319 [00:29<00:26, 31.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 19:16:11 metrics.py:334] Avg prompt throughput: 2557.2 tokens/s, Avg generation throughput: 3062.7 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 42 reqs, GPU KV cache usage: 6.4%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 593/1319 [00:35<00:36, 19.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 19:16:16 metrics.py:334] Avg prompt throughput: 2471.9 tokens/s, Avg generation throughput: 3024.7 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 42 reqs, GPU KV cache usage: 6.4%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 693/1319 [00:40<00:27, 22.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 19:16:21 metrics.py:334] Avg prompt throughput: 2326.0 tokens/s, Avg generation throughput: 2961.2 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 46 reqs, GPU KV cache usage: 6.6%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 805/1319 [00:45<00:16, 30.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 19:16:26 metrics.py:334] Avg prompt throughput: 2611.6 tokens/s, Avg generation throughput: 3027.2 tokens/s, Running: 253 reqs, Swapped: 0 reqs, Pending: 44 reqs, GPU KV cache usage: 6.6%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 908/1319 [00:50<00:21, 19.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 19:16:31 metrics.py:334] Avg prompt throughput: 2351.8 tokens/s, Avg generation throughput: 3084.1 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 51 reqs, GPU KV cache usage: 6.7%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 1002/1319 [00:55<00:17, 17.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 19:16:36 metrics.py:334] Avg prompt throughput: 2194.8 tokens/s, Avg generation throughput: 3026.8 tokens/s, Running: 252 reqs, Swapped: 0 reqs, Pending: 61 reqs, GPU KV cache usage: 7.0%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 1143/1319 [01:00<00:05, 31.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 19:16:41 metrics.py:334] Avg prompt throughput: 1417.6 tokens/s, Avg generation throughput: 3541.5 tokens/s, Running: 174 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.5%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 1300/1319 [01:03<00:00, 32.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 19:16:46 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2735.3 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 1311/1319 [01:09<00:03,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 19:16:51 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1079.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1314/1319 [01:10<00:01,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 19:16:56 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 574.4 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1319/1319 [01:19<00:00, 16.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter out results.\n",
      "Writing results to /cluster/work/lawecon/Work/mbraasch/output/dl_my_format/next_step_predictions.json.\n",
      "Writing final results to /cluster/work/lawecon/Work/mbraasch/output/dl_my_format/final_results.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_name = \"dl_my_format\"\n",
    "run_baseline_inference(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff311823-7e7e-4f18-87bd-71049ead80a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from calculate_metrics import calc_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc020f03-e344-4fd4-a190-8b08ffc2e729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "calc_metrics(\n",
    "    ground_truth_path=\"/cluster/work/lawecon/Work/mbraasch/data/formatted/test.json\",\n",
    "    output_dir=f\"/cluster/work/lawecon/Work/mbraasch/output/{run_name}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
