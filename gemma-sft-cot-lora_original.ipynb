{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "691881e2-8aff-48be-b991-46d612648be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 0: kill: (67472) - No such process\n"
     ]
    }
   ],
   "source": [
    "!kill -KILL 67472"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01fb89c7-d121-4e40-913d-7c5a635981f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun  3 19:04:46 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.07             Driver Version: 535.161.07   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:21:00.0 Off |                  Off |\n",
      "| 30%   33C    P5              48W / 450W |      1MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06b186eb-7c6d-4c66-b3c5-07d49701c8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No ROCm runtime is found, using ROCM_HOME='/opt/rocm'\n",
      "2024-06-03 19:04:53.722380: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd \n",
    "import random \n",
    "import transformers\n",
    "import datasets\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "MAX_SEQ_LENGTH = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "DTYPE = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "LOAD_IN_4BIT = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "MODEL_NAME = \"unsloth/gemma-2b-it\"\n",
    "DATA_PATH = \"/cluster/work/lawecon/Work/mbraasch/data/df_cot_kd.csv\"\n",
    "SEED = 42 \n",
    "random.seed(SEED) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9504347-5f5b-4459-9c8c-c05240254001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Gemma patching release 2024.5\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.65 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL_NAME,\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    dtype = DTYPE,\n",
    "    load_in_4bit = LOAD_IN_4BIT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba453e61-4c20-48d3-9496-9b3a4269f8f8",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54d93bc3-8738-4bf6-bdde-2ef1443e8ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to arrange dataframe such that the first n rows correspond to first n responses, and so on. \n",
    "# The idea is that the model sees all IDs first, instead of all responses from an ID \n",
    "def arrange_df(df, id_col='id', ques_col='question',response_col='response'):\n",
    "    df = df.sort_values(by=[id_col])\n",
    "    \n",
    "    # Group dataframe by 'id' column\n",
    "    grouped = df.groupby(id_col)\n",
    "    \n",
    "    # Initialize an empty list to store the arranged data\n",
    "    arranged_data = []\n",
    "    \n",
    "    # Get unique IDs and the maximum number of responses for any ID\n",
    "    unique_ids = df[id_col].unique()\n",
    "    max_responses = grouped.size().max()\n",
    "    \n",
    "    # Iterate over the number of responses (max_responses)\n",
    "    for i in range(max_responses):\n",
    "        # Iterate over unique IDs\n",
    "        for id_ in unique_ids:\n",
    "            # Get all responses and questions for the current ID\n",
    "            responses = grouped.get_group(id_)[response_col].tolist()\n",
    "            questions = grouped.get_group(id_)[ques_col].tolist()\n",
    "            # Append the ID, question, and response if available, else append None\n",
    "            if i < len(responses):\n",
    "                arranged_data.append({'id': id_, 'question': questions[i], 'response': responses[i]})\n",
    "            else:\n",
    "                arranged_data.append({'id': id_, 'question': None, 'response': None})\n",
    "    \n",
    "    # Create a new DataFrame from the arranged data\n",
    "    arranged_df = pd.DataFrame(arranged_data)\n",
    "    arranged_df = arranged_df.dropna(subset=['response'])\n",
    "    arranged_df.reset_index(drop=True, inplace=True)\n",
    "    print(arranged_df.head(10))\n",
    "    \n",
    "    assert(arranged_df.shape[0] == df.shape[0])\n",
    "    return arranged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e7ee50d-5173-4ac7-b089-4e153d6e6341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get Train and Val DataFrames - No Common IDs \n",
    "def split_train_val(df, ratio=0.8):\n",
    "    ids = list(set(df['id'].tolist()))\n",
    "    random.seed(SEED)\n",
    "    random.shuffle(ids)\n",
    "    \n",
    "    ntrain = int(ratio*len(ids))\n",
    "    train_ids = ids[:ntrain]\n",
    "    val_ids = ids[ntrain:]\n",
    "    \n",
    "    df_train = df[df['id'].isin(train_ids)].copy()\n",
    "    df_val = df[df['id'].isin(val_ids)].copy()\n",
    "    \n",
    "    print(\"Train shape: \", df_train.shape)\n",
    "    print(\"Val shape: \", df_val.shape)\n",
    "    print(\"Train-Val distribution: \", df_train.shape[0]/(df_train.shape[0] + df_val.shape[0]))\n",
    "    \n",
    "    return df_train, df_val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b53a26af-94b3-44f2-87ec-732635f9ec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(col_name, path=DATA_PATH, arrange_train=False, remove_train_duplicates=True, remove_val_duplicates=True, split_ratio=1.0):\n",
    "    df = pd.read_csv(path)\n",
    "    if 'id' not in df.columns:\n",
    "        df.reset_index(inplace=True, drop=False)\n",
    "        df.rename(columns={'index':'id'}, inplace=True)\n",
    "    \n",
    "    df.rename(columns={col_name:'response'}, inplace=True)\n",
    "    # Get Train and Val DFs\n",
    "    \n",
    "    df_train, df_val = split_train_val(df, split_ratio)\n",
    "    df_train = df_train.sort_values(by=['id'])\n",
    "    df_val = df_val.sort_values(by=['id'])\n",
    "    \n",
    "    if remove_train_duplicates:\n",
    "        print(\"Removing Train Duplicates\")\n",
    "        df_train = df_train.drop_duplicates(subset=['id'])\n",
    "        df_train.reset_index(drop=True, inplace=True) \n",
    "        print(df_train.shape)\n",
    "        print(df_train.head())\n",
    "    \n",
    "    # Arrange DF Train\n",
    "    if arrange_train:\n",
    "        df_train = arrange_df(df_train)\n",
    "    \n",
    "    # Remove Duplicates for Validation DF \n",
    "    if remove_val_duplicates:\n",
    "        #print(\"Removing Val Duplicates\")\n",
    "        df_val = df_val.drop_duplicates(subset=['id'])\n",
    "        df_val.reset_index(drop=True, inplace=True)\n",
    "        #print(df_val.shape)\n",
    "        #print(df_val.head())\n",
    "        \n",
    "    # Convert to Dataset \n",
    "    dataset_train = datasets.Dataset.from_pandas(df_train[['id','question','response']].copy())\n",
    "    dataset_val = datasets.Dataset.from_pandas(df_val[['id','question','response']].copy())\n",
    "    \n",
    "    # Dataset Dict\n",
    "    dataset_train = dataset_train.map(formatting_prompts_func, batched = True)\n",
    "    dataset_val = dataset_val.map(formatting_prompts_func, batched = True)\n",
    "    \n",
    "    ds = datasets.DatasetDict({\"train\":dataset_train, \"val\":dataset_val})\n",
    "    \n",
    "    ##print(ds)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a766c70-ab7b-4e6e-9888-1159815c6c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Format Text \n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Solve the following math word problem step-by-step.\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs       = examples[\"question\"]\n",
    "    outputs      = examples[\"response\"]\n",
    "    texts = []\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "885cf7c2-02e4-453b-b175-6d23fc447328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_gt(path, split_ratio = 0.0):\n",
    "    \"\"\"\n",
    "    Loads data from disk, formats the prompt and casts into the correct dataset structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data from disk\n",
    "    import json\n",
    "    with open(path, \"r\") as f:\n",
    "        my_data = json.load(f)\n",
    "\n",
    "    # Format prompt\n",
    "    train_data = [\n",
    "        {\n",
    "            \"text\": alpaca_prompt.format(\n",
    "                element[\"source_question\"],\n",
    "                element[\"target_steps\"] + f\"\\nFinal Answer: {element['target_result']}\" + tokenizer.eos_token\n",
    "            )\n",
    "        }\n",
    "        for element in my_data\n",
    "    ]\n",
    "\n",
    "    # Format into huggingface dataset\n",
    "    ds = datasets.Dataset.from_pandas(pd.DataFrame(data=train_data))\n",
    "    \n",
    "    if split_ratio:\n",
    "        ds = ds.train_test_split(split_ratio)\n",
    "        ds[\"val\"] = ds[\"test\"]\n",
    "        del ds[\"test\"]\n",
    "        print(\"Train:\")\n",
    "        print(ds[\"train\"])\n",
    "        print(\"Val:\")\n",
    "        print(ds[\"val\"])\n",
    "    else:\n",
    "        ds = datasets.DatasetDict({\"train\":ds})\n",
    "        print(\"Train:\")\n",
    "        print(ds[\"train\"])\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "366797ae-7d38-42d7-9a27-d8d89aa304fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 6228\n",
      "})\n",
      "Train shape:  (6228, 8)\n",
      "Val shape:  (0, 8)\n",
      "Train-Val distribution:  1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdcccbad684c48f68963780765eeebb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6228 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# My data\n",
    "OUTPUT_PATH = \"/cluster/work/lawecon/Work/mbraasch/output/dl_my_format\"\n",
    "DATA_PATH = \"/cluster/work/lawecon/Work/mbraasch/data/formatted/train_dl_my_format.json\"\n",
    "#DATA_PATH = \"/cluster/work/lawecon/Work/mbraasch/data/formatted/train_filtered.json\"\n",
    "split_ratio = 1.0\n",
    "\n",
    "data = prepare_data_gt(path=DATA_PATH, split_ratio=1-split_ratio)\n",
    "\n",
    "# Shivam data\n",
    "data = prepare_data(col_name='output_answer', arrange_train=False, remove_train_duplicates=False, remove_val_duplicates=True, split_ratio=split_ratio)\n",
    "\n",
    "#dataset_train = data[\"train\"]\n",
    "#dataset_val = data[\"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f5f875c-a19d-4406-bb9c-8e28a17ebcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    print(\"My data:\")\n",
    "    print(\"##############################\")\n",
    "    print(my_data[\"train\"][0][\"text\"])\n",
    "    print(\"##############################\\n\")\n",
    "    print(\"Shivam data:\")\n",
    "    print(\"##############################\")\n",
    "    print(sh_data[\"train\"][0][\"text\"])\n",
    "    print(\"##############################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efed4967-f345-46a0-9d38-3dc8201ecf9d",
   "metadata": {},
   "source": [
    "### LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ce563fa-2e26-46bd-af16-e1c3b3f654d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.5 patched 18 layers with 18 QKV layers, 18 O layers and 18 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = SEED,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97c745d5-6189-41b8-8bc6-2fff345e05f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /cluster/home/mbraasch/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=gemma-2b-it\n"
     ]
    }
   ],
   "source": [
    "import wandb \n",
    "## wandb variables\n",
    "wandb.login(relogin=True, key='edcec5761dfce8c0c60778393ae2f6ceba79df18')\n",
    "%env WANDB_PROJECT=gemma-2b-it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b775dc5-6dbd-451a-8434-55f82e84898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(\n",
    "    #evaluation_strategy = \"epoch\",\n",
    "    logging_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    #save_steps = \"epoch\",\n",
    "    #load_best_model_at_end=True,\n",
    "    # metric_for_best_model = \"loss\",\n",
    "    #logging_steps = 50,\n",
    "\n",
    "    per_device_train_batch_size = 8,\n",
    "    gradient_accumulation_steps = 1,\n",
    "    warmup_steps = 5,\n",
    "    num_train_epochs = 2,\n",
    "    learning_rate = 2e-4,\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    # optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"constant\",\n",
    "    seed = SEED,\n",
    "    output_dir = OUTPUT_PATH,\n",
    "    report_to = \"wandb\",\n",
    "    save_total_limit = None,\n",
    "    run_name=OUTPUT_PATH\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa4f4d73-e40c-49f8-bfc4-05d760cac589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = data[\"train\"],\n",
    "    #eval_dataset = data[\"val\"],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    # dataset_num_proc = 2,\n",
    "    packing = True, \n",
    "    args = train_args\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e19c405b-000b-4236-9ca0-347e5dd05150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcelbraasch\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cluster/home/mbraasch/sft/wandb/run-20240603_190507-4zesrrpg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/marcelbraasch/gemma-2b-it-lora-cot-no-duplicates/runs/4zesrrpg\" target=\"_blank\">trim-valley-60</a></strong> to <a href=\"https://wandb.ai/marcelbraasch/gemma-2b-it-lora-cot-no-duplicates\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/marcelbraasch/gemma-2b-it-lora-cot-no-duplicates/runs/4zesrrpg?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2b0e92087fa0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(settings=wandb.Settings(start_method='fork'), project='gemma-2b-it-lora-cot-no-duplicates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc11eca7-cb2f-4f97-8806-5932ad4413c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.65 GB.\n",
      "5.031 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63ac8b3f-4834-4bb3-8bab-6a06300800d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 1,481 | Num Epochs = 2\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 8 | Total steps = 372\n",
      " \"-____-\"     Number of trainable parameters = 19,611,648\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='372' max='372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [372/372 06:49, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.642800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372</td>\n",
       "      <td>0.512500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5b48449-3791-441e-b180-78722b5be6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>‚ñÅ‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÅ‚ñà</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>4.596555201065779e+16</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>372</td></tr><tr><td>train/grad_norm</td><td>1.2801</td></tr><tr><td>train/learning_rate</td><td>0.0002</td></tr><tr><td>train/loss</td><td>0.5125</td></tr><tr><td>train_loss</td><td>0.57764</td></tr><tr><td>train_runtime</td><td>412.6644</td></tr><tr><td>train_samples_per_second</td><td>7.178</td></tr><tr><td>train_steps_per_second</td><td>0.901</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">trim-valley-60</strong>: <a href=\"https://wandb.ai/marcelbraasch/gemma-2b-it-lora-cot-no-duplicates/runs/4zesrrpg\" target=\"_blank\">https://wandb.ai/marcelbraasch/gemma-2b-it-lora-cot-no-duplicates/runs/4zesrrpg</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240603_190507-4zesrrpg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c0c684-ee55-4166-8323-806f6388eaf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
