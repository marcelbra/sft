{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "experiment_name = \"/cluster/work/lawecon/Work/mbraasch/output/TEC\"\n",
    "\n",
    "# Find all paths that end with directory \"m2345678\", \"m3456789\", \"m456789\", \"m12345678\"\n",
    "runs = list(range(1, 6))\n",
    "experts = [\"m2345678\", \"m345678\", \"m45678\"]\n",
    "datasets = [\"socratic\", \"dl\", \"gt\"]\n",
    "models = [\n",
    "    \"microsoft--Phi-3-mini-128k-instruct\",\n",
    "    \"google--gemma-1.1-2b-it\",\n",
    "    \"mistralai--Mistral-7B-Instruct-v0.3\",\n",
    "    \"deepseek-ai--deepseek-llm-7b-chat\"\n",
    "]\n",
    "postfix = \"_OSC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft--Phi-3-mini-128k-instruct socratic m2345678\n",
      "0.6849128127369218\n",
      "microsoft--Phi-3-mini-128k-instruct socratic m345678\n",
      "0.6905231235784686\n",
      "microsoft--Phi-3-mini-128k-instruct socratic m45678\n",
      "0.6773313115996968\n",
      "microsoft--Phi-3-mini-128k-instruct dl m2345678\n",
      "0.730705079605762\n",
      "microsoft--Phi-3-mini-128k-instruct dl m345678\n",
      "0.7253980288097044\n",
      "microsoft--Phi-3-mini-128k-instruct dl m45678\n",
      "0.728885519332828\n",
      "google--gemma-1.1-2b-it socratic m2345678\n",
      "0.2597422289613343\n",
      "google--gemma-1.1-2b-it socratic m345678\n",
      "0.20242608036391205\n",
      "google--gemma-1.1-2b-it socratic m45678\n",
      "0.22926459438968916\n",
      "google--gemma-1.1-2b-it dl m2345678\n",
      "0.3153904473085671\n",
      "google--gemma-1.1-2b-it dl m345678\n",
      "0.2838514025777104\n",
      "google--gemma-1.1-2b-it dl m45678\n",
      "0.29006823351023503\n",
      "google--gemma-1.1-2b-it gt m2345678\n",
      "0.2582259287338893\n",
      "google--gemma-1.1-2b-it gt m345678\n",
      "0.23487490523123578\n",
      "google--gemma-1.1-2b-it gt m45678\n",
      "0.2391205458680819\n",
      "mistralai--Mistral-7B-Instruct-v0.3 socratic m2345678\n",
      "0.35405610310841545\n",
      "mistralai--Mistral-7B-Instruct-v0.3 socratic m345678\n",
      "0.31235784685367707\n",
      "mistralai--Mistral-7B-Instruct-v0.3 socratic m45678\n",
      "0.3449583017437453\n",
      "mistralai--Mistral-7B-Instruct-v0.3 dl m2345678\n",
      "0.4309325246398787\n",
      "mistralai--Mistral-7B-Instruct-v0.3 dl m345678\n",
      "0.4165276724791509\n",
      "mistralai--Mistral-7B-Instruct-v0.3 dl m45678\n",
      "0.4330553449583017\n",
      "mistralai--Mistral-7B-Instruct-v0.3 gt m2345678\n",
      "0.3630022744503411\n",
      "mistralai--Mistral-7B-Instruct-v0.3 gt m345678\n",
      "0.32100075815011375\n",
      "mistralai--Mistral-7B-Instruct-v0.3 gt m45678\n",
      "0.34283548142532216\n",
      "deepseek-ai--deepseek-llm-7b-chat socratic m2345678\n",
      "0.4692949203942381\n",
      "deepseek-ai--deepseek-llm-7b-chat socratic m345678\n",
      "0.47141774071266107\n",
      "deepseek-ai--deepseek-llm-7b-chat socratic m45678\n",
      "0.4727824109173616\n",
      "deepseek-ai--deepseek-llm-7b-chat dl m2345678\n",
      "0.558756633813495\n",
      "deepseek-ai--deepseek-llm-7b-chat dl m345678\n",
      "0.5466262319939348\n",
      "deepseek-ai--deepseek-llm-7b-chat dl m45678\n",
      "0.5573919636087946\n",
      "deepseek-ai--deepseek-llm-7b-chat gt m2345678\n",
      "0.47945413191811986\n",
      "deepseek-ai--deepseek-llm-7b-chat gt m345678\n",
      "0.47141774071266107\n",
      "deepseek-ai--deepseek-llm-7b-chat gt m45678\n",
      "0.469598180439727\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    for dataset in datasets:\n",
    "        for expert in experts:\n",
    "\n",
    "            if model == \"microsoft--Phi-3-mini-128k-instruct\" and dataset == \"gt\":\n",
    "                continue\n",
    "                # broken\n",
    "\n",
    "            accs = []\n",
    "            for run in runs:\n",
    "                file_path = os.path.join(experiment_name, model, dataset, expert, str(run), f\"accuracy{postfix}.json\")\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    accs.append(json.load(f)[\"accuracy\"])\n",
    "            avg_acc = sum(accs) / len(accs)\n",
    "            \n",
    "            # Write to the output file\n",
    "            output_file = os.path.join(experiment_name, model, dataset, expert, f\"accuracy{postfix}.json\")\n",
    "            with open(output_file, \"w\") as f:\n",
    "                json.dump({\"accuracy\": avg_acc}, f)\n",
    "\n",
    "            # Print accuracy to the console for easy copy-pasting\n",
    "            print(model, dataset, expert)\n",
    "            print(avg_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
